{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Modelling and evaluation </h1>\n",
    "<h2> 1. Import and download </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score as ACC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import RNN, Dense, Dropout, BatchNormalization\n",
    "from keras import Sequential, layers, Input, callbacks\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the datasets\n",
    "train_A = pd.read_csv('data/train_A.csv')\n",
    "train_B = pd.read_csv('data/train_B.csv')\n",
    "train_C = pd.read_csv('data/train_C.csv')\n",
    "\n",
    "val_A = pd.read_csv('data/val_A.csv')\n",
    "val_B = pd.read_csv('data/val_B.csv')\n",
    "val_C = pd.read_csv('data/val_C.csv')\n",
    "\n",
    "test_A = pd.read_csv('data/test_A.csv')\n",
    "test_B = pd.read_csv('data/test_B.csv')\n",
    "test_C = pd.read_csv('data/test_C.csv')\n",
    "\n",
    "train_D = pd.read_csv('data/train_D.csv')\n",
    "val_D= pd.read_csv('data/val_D.csv')\n",
    "test_D = pd.read_csv('data/test_D.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Data preprocessing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "            train_A, val_A, test_A, \n",
    "            # train_B, val_B, test_B, \n",
    "            train_C, val_C, test_C,\n",
    "            train_D, val_D, test_D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char</th>\n",
       "      <th>dialog</th>\n",
       "      <th>word_len</th>\n",
       "      <th>character_len</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>propn_count</th>\n",
       "      <th>uinque_words</th>\n",
       "      <th>dialog_sentiment</th>\n",
       "      <th>...</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>disgust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>sadness</th>\n",
       "      <th>avg_tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rest</td>\n",
       "      <td>Grond, Grond, Grond, Grond!</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRODO</td>\n",
       "      <td>Smeagol?</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAM</td>\n",
       "      <td>Look!The gate.It's opening!I can see a way down.</td>\n",
       "      <td>8</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.286746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIPPIN</td>\n",
       "      <td>Well, that's good news.</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.231094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIPPIN</td>\n",
       "      <td>Frodo.</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>Rest</td>\n",
       "      <td>Your words are poison.</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.800083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>Rest</td>\n",
       "      <td>A new power is rising.Its victory is at hand.</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.817054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>Rest</td>\n",
       "      <td>Come on then , come on!</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.375</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.175262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>SAM</td>\n",
       "      <td>Mr. Frodo!</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>ARAGORN</td>\n",
       "      <td>Send out riders, my lord. You must call for ai...</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.790384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1672 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         char                                             dialog  word_len  \\\n",
       "0        Rest                      Grond, Grond, Grond, Grond!           4   \n",
       "1       FRODO                                         Smeagol?           1   \n",
       "2         SAM  Look!The gate.It's opening!I can see a way down.          8   \n",
       "3      PIPPIN                           Well, that's good news.          4   \n",
       "4      PIPPIN                                            Frodo.          1   \n",
       "...       ...                                                ...       ...   \n",
       "1667     Rest                            Your words are poison.          4   \n",
       "1668     Rest     A new power is rising.Its victory is at hand.          9   \n",
       "1669     Rest                      Come on then , come on!               6   \n",
       "1670      SAM                                        Mr. Frodo!          2   \n",
       "1671  ARAGORN  Send out riders, my lord. You must call for ai...        10   \n",
       "\n",
       "      character_len  stopword_count  verb_count  adj_count  propn_count  \\\n",
       "0                29               0           0          0            4   \n",
       "1                10               0           0          0            0   \n",
       "2                49               2           1          0            0   \n",
       "3                24               0           0          1            0   \n",
       "4                11               0           0          0            1   \n",
       "...             ...             ...         ...        ...          ...   \n",
       "1667             27               2           0          0            0   \n",
       "1668             46               4           1          1            0   \n",
       "1669             31               2           2          0            0   \n",
       "1670             11               0           0          0            2   \n",
       "1671             50               4           2          0            0   \n",
       "\n",
       "      uinque_words  dialog_sentiment  ...  fear  anger  positive  negative  \\\n",
       "0                2             0.000  ...     0      0         0         0   \n",
       "1                1             0.000  ...     0      0         0         0   \n",
       "2                8             0.125  ...     0      0         0         0   \n",
       "3                4             0.500  ...     0      0         0         0   \n",
       "4                1             0.000  ...     0      0         0         0   \n",
       "...            ...               ...  ...   ...    ...       ...       ...   \n",
       "1667             4            -0.125  ...     0      2         0         0   \n",
       "1668             8             1.500  ...     0      0         0         0   \n",
       "1669             6             0.375  ...     0      0         0         0   \n",
       "1670             2             0.000  ...     0      0         0         0   \n",
       "1671            10             0.125  ...     0      0         1         0   \n",
       "\n",
       "      disgust  surprise  joy  anticipation  sadness  avg_tf-idf  \n",
       "0           0         0    0             0        0    0.000000  \n",
       "1           0         0    0             0        0    0.000000  \n",
       "2           0         0    0             0        0    1.286746  \n",
       "3           0         0    0             1        0    1.231094  \n",
       "4           0         0    0             0        0    0.000000  \n",
       "...       ...       ...  ...           ...      ...         ...  \n",
       "1667        0         0    0             0        0    1.800083  \n",
       "1668        0         0    0             1        0    0.817054  \n",
       "1669        0         0    0             0        0    2.175262  \n",
       "1670        0         0    0             0        0    0.000000  \n",
       "1671        1         0    0             0        0    0.790384  \n",
       "\n",
       "[1672 rows x 22 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "imp_char = [\"FRODO\", \"SAM\", \"GANDALF\", \"PIPPIN\", \"MERRY\", \"GOLLUM\", \"GIMLI\", \"THEODEN\", \"FARAMIR\", \"ARAGORN\"]\n",
    "\n",
    "# Creating a common label for the characters not of interest\n",
    "def common_label_removal(data):\n",
    "    mask = data[\"char\"].isin(imp_char)\n",
    "    data.loc[~ mask, \"char\"] = \"Rest\"\n",
    "    mask2 = data['char'] == 'Rest'\n",
    "    data = data[~mask2]\n",
    "    return data\n",
    "\n",
    "def x_y_split(data):\n",
    "    y_data = data['char']\n",
    "    x_data = data.drop(columns=['char', 'dialog'])\n",
    "    return x_data, y_data\n",
    "\n",
    "def char_2_num(y_data):\n",
    "    encoder = LabelEncoder()\n",
    "    y_data = y_data.values.reshape(-1, 1)\n",
    "    encoded_data = encoder.fit_transform(y_data)\n",
    "    names = list(encoder.inverse_transform(np.unique(encoded_data)))\n",
    "    print(names)\n",
    "    print(np.unique(encoded_data))\n",
    "    return encoded_data, names\n",
    "\n",
    "def preprocessing(data):\n",
    "    data = common_label_removal(data)\n",
    "    x_data, y_data = x_y_split(data)\n",
    "    y_data = char_2_num(y_data)\n",
    "    return x_data, y_data\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    datasets[i] = preprocessing(datasets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_tra_X =datasets[0][0]\n",
    "A_tra_y =datasets[0][1][0]\n",
    "A_val_X =datasets[1][0]\n",
    "A_val_y=datasets[1][1][0]\n",
    "A_tar_X=datasets[2][0]\n",
    "A_tar_y=datasets[2][1][0]\n",
    "\n",
    "C_tra_X =datasets[3][0]\n",
    "C_tra_y =datasets[3][1][0]\n",
    "C_val_X =datasets[4][0]\n",
    "C_val_y=datasets[4][1][0]\n",
    "C_tar_X=datasets[5][0]\n",
    "C_tar_y=datasets[5][1][0]\n",
    "\n",
    "D_tra_X =datasets[6][0]\n",
    "D_tra_y =datasets[6][1][0]\n",
    "D_val_X =datasets[7][0]\n",
    "D_val_y=datasets[7][1][0]\n",
    "D_tar_X=datasets[8][0]\n",
    "D_tar_y=datasets[8][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_val_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Benchmarks </h2>\n",
    "<h3> 2.1 Naive Benchmark, Monte Carlo Method </h3>\n",
    "<p> Using 1000 simulations with random guesses on target labels. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_benchmark_MonC(y):\n",
    "    accuracy_list = []\n",
    "    for i in range(0,1000,1):\n",
    "        naive_rand_pred = np.random.randint(0,12,size=(len(y)))\n",
    "        accuracy_sel = ACC(naive_rand_pred, y)\n",
    "        accuracy_list.append(accuracy_sel)\n",
    "    return np.mean(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08334188034188034"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_benchmark_MonC(A_tar_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2 Naive Benchmark, Majority Class Method </h3>\n",
    "<p> Using Frodo, which equals label 2, as guess </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_benchmark_MajC(y):\n",
    "    pred_MCNB =np.repeat(2,len(y))\n",
    "    return ACC(pred_MCNB, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1752136752136752"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_benchmark_MajC(A_tar_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Modelling  </h2>\n",
    "<h3> 3.1 ANN on dataset A</h3>\n",
    "<p> Dataset A contains various numerical retrieved from the characters. </p>\n",
    "<p> The feedforward neural network has a relative simple architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# A1 = scaler.fit_transform(D_tra_X)\n",
    "# A2 = scaler.transform(D_val_X)\n",
    "# A3 = scaler.transform(D_tar_X)\n",
    "A1 = D_tra_X.reset_index(drop=True)\n",
    "A2 = D_val_X.reset_index(drop=True)\n",
    "A3 = D_tar_X.reset_index(drop=True)\n",
    "\n",
    "Y1 = np.eye(10)[D_tra_y]\n",
    "Y2 = np.eye(10)[D_val_y]\n",
    "Y3 = np.eye(10)[D_tar_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann_model = keras.Sequential([\n",
    "#     layers.Dense(8, activation='relu',input_dim=20),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(rate=0.3),\n",
    "#     # layers.Dense(16, activation='selu'),\n",
    "#     # layers.BatchNormalization(),\n",
    "#     # layers.Dropout(0.3),\n",
    "#     layers.Dense(10, activation='softmax'),\n",
    "#     layers.Dense(10)\n",
    "# ])\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "# ann_model.compile(optimizer=optimizer,\n",
    "#               loss = 'categorical_crossentropy',\n",
    "#               metrics=['accuracy']\n",
    "#               )\n",
    "\n",
    "# early_stopping = callbacks.EarlyStopping(\n",
    "#     min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "#     patience=35, # how many epochs to wait before stopping\n",
    "#     restore_best_weights=True,\n",
    "# )\n",
    "# ann_model.fit(A1, Y1, \n",
    "#           validation_data= (A2, Y2),\n",
    "#           epochs=200, batch_size=10, \n",
    "#           callbacks=early_stopping,\n",
    "#           verbose=0\n",
    "#           )\n",
    "\n",
    "# print('Accuracy train: ',ann_model.evaluate(A1, Y1))\n",
    "# print('Accuracy validation: ',ann_model.evaluate(A2, Y2))\n",
    "# print('Accuracy test: ',ann_model.evaluate(A3, Y3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier \n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_g = {\n",
    "    'objective':['multi:softprob'],\n",
    "    'alpha': hp.uniform('alpha',0,1),\n",
    "    'gamma': hp.uniform('gamma',0,9),\n",
    "    'reg_lambda':hp.quniform('reg_lamda',0,3,1),\n",
    "    'max_depth':hp.quniform('max_depth',6,12,1),\n",
    "    'learning_rate': hp.uniform('learning_rate',0.001,0.05),\n",
    "    'n_estimators': hp.quniform('n_estimators', 5,500,1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight',0,5,1),\n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "    'seed':42\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1147, 2697)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tar_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:                                               \n",
      "0.25978355800265807                                  \n",
      "Score:                                                                           \n",
      "0.25803303588380483                                                              \n",
      "Score:                                                                           \n",
      "0.2981469527245111                                                               \n",
      "Score:                                                                           \n",
      "0.2615264856654642                                                              \n",
      "Score:                                                                          \n",
      "0.26501613821910003                                                             \n",
      "100%|██████████| 5/5 [01:49<00:00, 21.95s/trial, best loss: -0.2981469527245111]\n"
     ]
    }
   ],
   "source": [
    "def bayopt_xgb(p_g):\n",
    "    internal_model = XGBClassifier(\n",
    "                     objective='multi:softprob',\n",
    "                     alpha=p_g['alpha'],\n",
    "                     gamma=p_g['gamma'],\n",
    "                     reg_lambda= p_g['reg_lambda'],\n",
    "                    #  colsample_bytree= p_q['colsample_bytree'],\n",
    "                     max_depth = int(p_g['max_depth']),\n",
    "                     n_estimator = (p_g['n_estimators']),\n",
    "                     learning_rate=p_g['learning_rate'],\n",
    "                    #  min_child_weight=p_g['min_child_weight'],\n",
    "                     seed =p_g['seed'],\n",
    "                     )\n",
    "    # evaluation = [(A2, A_val_y)]\n",
    "\n",
    "    internal_model.fit(A1, D_tra_y,\n",
    "                     eval_set = [(A2, D_val_y)],\n",
    "                     eval_metric = 'mlogloss',\n",
    "                     early_stopping_rounds=25,verbose=False)\n",
    "    \n",
    "    # pred_valid = internal_model.predict(A2)\n",
    "    # score = ACC(pred_valid, A_tra_y)\n",
    "\n",
    "    score =np.mean(cross_val_score(internal_model, A1, D_tra_y, scoring='accuracy', cv=5))\n",
    "    print('Score:', score)\n",
    "    return {'loss':-score, 'status':STATUS_OK}\n",
    "\n",
    "def tune():\n",
    "    trials = Trials()\n",
    "    best_tune = fmin(fn=bayopt_xgb, \n",
    "                    space=p_g,\n",
    "                    algo= tpe.suggest,\n",
    "                    max_evals=5,\n",
    "                    trials=trials)\n",
    "    return best_tune\n",
    "\n",
    "\n",
    "ntune = tune()\n",
    "ntune['n_estimators'] =  int(ntune['n_estimators'])\n",
    "ntune['max_depth'] =  int(ntune['max_depth'])\n",
    "xmodel = XGBClassifier(**ntune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(alpha=0.4100022157807687, base_score=None, booster=None,\n",
       "              callbacks=None, colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.6293405193972841, device=None,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=1.2507628960845056,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.04696753195848873,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=12, max_leaves=None,\n",
       "              min_child_weight=3.0, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=142, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(alpha=0.4100022157807687, base_score=None, booster=None,\n",
       "              callbacks=None, colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.6293405193972841, device=None,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=1.2507628960845056,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.04696753195848873,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=12, max_leaves=None,\n",
       "              min_child_weight=3.0, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=142, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(alpha=0.4100022157807687, base_score=None, booster=None,\n",
       "              callbacks=None, colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.6293405193972841, device=None,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=1.2507628960845056,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.04696753195848873,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=12, max_leaves=None,\n",
       "              min_child_weight=3.0, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=142, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmodel.fit(A1, D_tra_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train:  0.37401918047079336\n",
      "Accuracy validation:  0.2874015748031496\n",
      "Accuracy test:  0.27350427350427353\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy train: ',ACC(xmodel.predict(A1),D_tra_y))\n",
    "print('Accuracy validation: ',ACC(xmodel.predict(A2),D_val_y))\n",
    "print('Accuracy test: ',ACC(xmodel.predict(A3),D_tar_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal_model = XGBClassifier(\n",
    "#                             objective='multi:softmax',\n",
    "#                                 #  alpha=p_q['alpha'],\n",
    "#                                 #  gamma=p_q['gamma'],\n",
    "#                                 #  reg_lambda= p_q['reg_lambda'],\n",
    "#                                 #  colsample_bytree= p_q['colsample_bytree'],\n",
    "#                             # max_depth = int(p_g['max_depth']),\n",
    "#                             max_depth = int(3),\n",
    "\n",
    "#                             n_estimator = (p_g['n_estimators']),\n",
    "#                             learning_rate=p_g['learning_rate'],\n",
    "#                             #  min_child_weight=p_g['min_child_weight'],\n",
    "#                             seed =p_g['seed'],\n",
    "#                             )\n",
    "# evaluation = [(A2, A_val_y)]\n",
    "\n",
    "# internal_model.fit(A1, A_tra_y,\n",
    "#                 eval_set = evaluation,\n",
    "#                 eval_metric = 'mlogloss',\n",
    "#                 early_stopping_rounds=25,verbose=False)\n",
    "    \n",
    "# pred_valid = internal_model.predict(A2)\n",
    "# score = ACC(A2, A_tra_y)\n",
    "#     # return pred_valid\n",
    "\n",
    "# print('Score:', score)\n",
    "# {'loss':-score, 'status':STATUS_OK}\n",
    "\n",
    "# def tune():\n",
    "#     trials = Trials()\n",
    "#     best_tune = fmin(fn=internal_model, \n",
    "#                     space=p_g,\n",
    "#                     algo= tpe.suggest,\n",
    "#                     max_evals=200,\n",
    "#                     trials=trials)\n",
    "#     return best_tune\n",
    "\n",
    "\n",
    "# ntune = tune()\n",
    "# ntune['n_estimators'] =  int(ntune['n_estimators'])\n",
    "# ntune['max_depth'] =  int(ntune['max_depth'])\n",
    "# # xmodel = XGBClassifier(**ntune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def cvscore():\n",
    "#     ntune = tune()\n",
    "#     ntune['n_estimators'] =  int(ntune['n_estimators'])\n",
    "#     ntune['max_depth'] =  int(ntune['max_depth'])\n",
    "#     xmodel = XGBClassifier(**ntune, random_state=42)\n",
    "#     cvs = cross_val_score(xmodel, A1, Y1, cv=25,\n",
    "#                          random_state=42)\n",
    "#     cvs.predict\n",
    "#     return cvs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.2 RNN on dataset B </h3>\n",
    "<p> Dataset B contains embeddings(?). This, I need to read myself up on.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Embedding,Dense\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = pd.read_csv('data/train_df.csv')\n",
    "B2= pd.read_csv('data/val_df.csv')\n",
    "B3 = pd.read_csv('data/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = common_label_removal(B1).reset_index(drop=True)\n",
    "B2 = common_label_removal(B2).reset_index(drop=True)\n",
    "B3 = common_label_removal(B3).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quote_list(X):\n",
    "    quote_list = []\n",
    "    for quote in range(len(X)):\n",
    "        splitted_quote =  X['dialog'][quote].split()\n",
    "        sequence_list = []\n",
    "        for split in range(len(splitted_quote)):\n",
    "            splitted_word = splitted_quote[split]\n",
    "\n",
    "            word_list = str()\n",
    "            i=0\n",
    "            while i < (len(splitted_word)):\n",
    "                # print(splitted_word[i])\n",
    "                if splitted_word[i].isalpha() == True:\n",
    "                    word_list += splitted_word[i]\n",
    "                i+=1\n",
    "            sequence_list.append(word_list)\n",
    "        quote_list.append(sequence_list)\n",
    "    return quote_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxlen(X):\n",
    "    uni = []\n",
    "    for i in range(len(quote_list)):\n",
    "        for j in range(len(quote_list[i])):\n",
    "            if quote_list[i][j] not in uni:\n",
    "                uni.append(quote_list[i][j])\n",
    "    return len(uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = quote_list(B1)\n",
    "B2 = quote_list(B2)\n",
    "B3 = quote_list(B3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(B1)\n",
    "B1_seq = tokenizer.texts_to_sequences(B1)\n",
    "B2_seq = tokenizer.texts_to_sequences(B2)\n",
    "B3_seq = tokenizer.texts_to_sequences(B3)\n",
    "maxlen = max([len(seq) for seq in B1_seq])\n",
    "\n",
    "B1_padseq = pad_sequences(B1_seq, maxlen=maxlen,padding='post')\n",
    "B2_padseq = pad_sequences(B2_seq, maxlen=maxlen,padding='post')\n",
    "B3_padseq = pad_sequences(B3_seq, maxlen=maxlen,padding='post')\n",
    "\n",
    "B1y = np.eye(10)[C_tra_y]\n",
    "B2y = np.eye(10)[C_val_y]\n",
    "B3y = np.eye(10)[C_tar_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.1211 - loss: 2.3122 - val_accuracy: 0.1299 - val_loss: 2.2466\n",
      "Epoch 2/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.1169 - loss: 2.3241 - val_accuracy: 0.1299 - val_loss: 2.2378\n",
      "Epoch 3/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.1275 - loss: 2.2900 - val_accuracy: 0.1693 - val_loss: 2.2303\n",
      "Epoch 4/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1106 - loss: 2.3082 - val_accuracy: 0.1496 - val_loss: 2.2368\n",
      "Epoch 5/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.1352 - loss: 2.2701 - val_accuracy: 0.1496 - val_loss: 2.2372\n",
      "Epoch 6/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1356 - loss: 2.2862 - val_accuracy: 0.1299 - val_loss: 2.2430\n",
      "Epoch 7/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1344 - loss: 2.2688 - val_accuracy: 0.1299 - val_loss: 2.2655\n",
      "Epoch 8/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1537 - loss: 2.2618 - val_accuracy: 0.0984 - val_loss: 2.2851\n",
      "Epoch 9/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1543 - loss: 2.2594 - val_accuracy: 0.1535 - val_loss: 2.2434\n",
      "Epoch 10/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.1176 - loss: 2.3001 - val_accuracy: 0.0827 - val_loss: 4.5119\n",
      "Epoch 11/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1582 - loss: 2.2507 - val_accuracy: 0.0827 - val_loss: 16.2724\n",
      "Epoch 12/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1539 - loss: 2.2561 - val_accuracy: 0.0827 - val_loss: 9.8834\n",
      "Epoch 13/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1619 - loss: 2.2377 - val_accuracy: 0.0827 - val_loss: 12.9663\n",
      "Epoch 14/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1395 - loss: 2.2562 - val_accuracy: 0.0827 - val_loss: 4.5123\n",
      "Epoch 15/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.1554 - loss: 2.2489 - val_accuracy: 0.1693 - val_loss: 11.7837\n",
      "Epoch 16/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1827 - loss: 2.2126 - val_accuracy: 0.1299 - val_loss: 17.5442\n",
      "Epoch 17/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1503 - loss: 2.2162 - val_accuracy: 0.1535 - val_loss: 3.6795\n",
      "Epoch 18/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1896 - loss: 2.1530 - val_accuracy: 0.0827 - val_loss: 41.1934\n",
      "Epoch 19/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1303 - loss: 2.2868 - val_accuracy: 0.0827 - val_loss: 30.7124\n",
      "Epoch 20/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.1266 - loss: 2.2751 - val_accuracy: 0.0984 - val_loss: 17.5176\n",
      "Epoch 21/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.1482 - loss: 2.2614 - val_accuracy: 0.0984 - val_loss: 16.9226\n",
      "Epoch 22/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1455 - loss: 2.2470 - val_accuracy: 0.0984 - val_loss: 16.1050\n",
      "Epoch 23/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1267 - loss: 2.2573 - val_accuracy: 0.0827 - val_loss: 8.3840\n",
      "Epoch 24/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1315 - loss: 2.2538 - val_accuracy: 0.1299 - val_loss: 13.7402\n",
      "Epoch 25/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1640 - loss: 2.2691 - val_accuracy: 0.1339 - val_loss: 4.7887\n",
      "Epoch 26/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.1812 - loss: 2.2149 - val_accuracy: 0.1299 - val_loss: 2.8461\n",
      "Epoch 27/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.1296 - loss: 2.2487 - val_accuracy: 0.1496 - val_loss: 2.3628\n",
      "Epoch 28/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.1593 - loss: 2.2565 - val_accuracy: 0.0827 - val_loss: 2.8331\n",
      "Epoch 29/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1479 - loss: 2.2368 - val_accuracy: 0.1299 - val_loss: 2.5134\n",
      "Epoch 30/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1483 - loss: 2.2590 - val_accuracy: 0.0787 - val_loss: 2.7431\n",
      "Epoch 31/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1645 - loss: 2.2385 - val_accuracy: 0.0866 - val_loss: 2.6775\n",
      "Epoch 32/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.1384 - loss: 2.2347 - val_accuracy: 0.0866 - val_loss: 4.3195\n",
      "Epoch 33/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.1486 - loss: 2.2315 - val_accuracy: 0.0866 - val_loss: 3.7250\n",
      "Epoch 34/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1432 - loss: 2.2506 - val_accuracy: 0.0866 - val_loss: 3.0646\n",
      "Epoch 35/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1635 - loss: 2.2252 - val_accuracy: 0.0866 - val_loss: 5.0154\n",
      "Epoch 36/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.1637 - loss: 2.2324 - val_accuracy: 0.0827 - val_loss: 5.6432\n",
      "Epoch 37/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.1844 - loss: 2.2334 - val_accuracy: 0.0827 - val_loss: 6.9543\n",
      "Epoch 38/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.1450 - loss: 2.2318 - val_accuracy: 0.0866 - val_loss: 4.7109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20d078c4090>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model = Sequential([\n",
    "    layers.Embedding(input_dim=2500, output_dim=15, input_length=maxlen),\n",
    "    # layers.Flatten(),\n",
    "    layers.LSTM(8,activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0,3),\n",
    "    layers.Dense(32, activation='selu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='gelu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
    "emb_model.compile(optimizer=optimizer, \n",
    "            loss='categorical_crossentropy', \n",
    "            metrics=['accuracy'])\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=35, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "emb_model.fit(B1_padseq,B1y, epochs=100, batch_size=30, \n",
    "        validation_data=(B2_padseq, B2y),\n",
    "        callbacks=early_stopping,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86\u001b[0m, \u001b[38;5;34m15\u001b[0m)         │        \u001b[38;5;34m37,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m768\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m288\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,788</span> (487.46 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,788\u001b[0m (487.46 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">41,526</span> (162.21 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m41,526\u001b[0m (162.21 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> (832.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m208\u001b[0m (832.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83,054</span> (324.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m83,054\u001b[0m (324.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1199 - loss: 2.2595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.258237361907959, 0.12205754220485687]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy\n",
    "emb_model.evaluate(B1_padseq, B1y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1400 - loss: 2.2245 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2302918434143066, 0.16929133236408234]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation accuracy\n",
    "emb_model.evaluate(B2_padseq, B2y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1632 - loss: 2.2011 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2168354988098145, 0.1367521435022354]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test accuracy\n",
    "emb_model.evaluate(B3_padseq, B3y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> sources </p>\n",
    "<ul>\n",
    "<li>https://keras.io/api/models/model/</li>\n",
    "<li>https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456</li>\n",
    "<li>https://www.kaggle.com/code/rajmehra03/a-detailed-explanation-of-keras-embedding-layer</li>\n",
    "<li>https://medium.com/@iqra.bismi/understanding-keras-embedding-for-natural-language-processing-9f65a281b1a7</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.3 RFC on dataset C </h3>\n",
    "<p>  Dataset C contains a counter on how many times a specific word have been mentioned in a quote. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [30,35,45,55,65,75,85,95],\n",
    "    'max_depth': [6,9,12,15,18,21,24,27,30],\n",
    "}\n",
    "\n",
    "acc_list = []\n",
    "for n in range(len(param_grid['n_estimators'])):\n",
    "    nE = param_grid['n_estimators'][n]\n",
    "    for d in range(len(param_grid['max_depth'])):\n",
    "        mD = param_grid['max_depth'][d]\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=nE, max_depth=mD, random_state=42) \n",
    "        model.fit(C_tra_X,C_tra_y)\n",
    "        X1 = model.predict(C_tra_X)\n",
    "        x2 = model.predict(C_val_X)\n",
    "        acc_list.append(ACC(x2, C_val_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([59], dtype=int64),)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series(acc_list)\n",
    "np.where(a==max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ne 85\n",
    "#md 24\n",
    "rfc_model = RandomForestClassifier(n_estimators=55, max_depth=15,random_state=42)\n",
    "rfc_model.fit(C_tra_X,C_tra_y)\n",
    "predCtrain= rfc_model.predict(C_tra_X)\n",
    "predCval= rfc_model.predict(C_val_X)\n",
    "predCtest= rfc_model.predict(C_tar_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5483870967741935"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy \n",
    "ACC(predCtrain, C_tra_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2952755905511811"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy \n",
    "ACC(predCval, C_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3247863247863248"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy \n",
    "ACC(predCtest, C_tar_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Ensemble model </h2>\n",
    "<p> The RFC contains absolutely best results therefore, they will have prioritized votes if there are ties. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann_model\n",
    "# emb_model\n",
    "# rfc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n"
     ]
    }
   ],
   "source": [
    "P1 = xmodel.predict(A3)\n",
    "# P1 = pp.argmax(axis=1)\n",
    "\n",
    "pp = emb_model.predict(B3_padseq)\n",
    "P2 = pp.argmax(axis=1)\n",
    "\n",
    "P3 = rfc_model.predict(C_tar_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = []\n",
    "for i in range(len(P1)):\n",
    "    preds =  [P1[i],P2[i],P3[i]]\n",
    "    if preds[0]==preds[1]:\n",
    "        ans = preds[0]\n",
    "    elif preds[0]==preds[2]:\n",
    "        ans= preds[0]\n",
    "    elif preds[1]==preds[2]:\n",
    "        ans=preds[1]\n",
    "    else:\n",
    "        ans = preds[2]\n",
    "    final_preds.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32905982905982906"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACC(final_preds, A_tar_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "alfa = B1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bravo = str()\n",
    "for i in range(len(alfa)):\n",
    "    bravo +=' '\n",
    "    bravo += str(alfa[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' LookThe gateIts openingI can see a way down'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bravo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "APtrain =  pd.DataFrame(xmodel.predict_proba(A1))\n",
    "BPtrain =  pd.DataFrame(emb_model.predict(B1_padseq))\n",
    "CPtrain =  pd.DataFrame(rfc_model.predict_proba(C_tra_X))\n",
    "prob_train = pd.concat([APtrain, BPtrain, CPtrain], axis=1)\n",
    "prob_train.columns = [i for i in range(30)]\n",
    "\n",
    "# Validation\n",
    "APval =  pd.DataFrame(xmodel.predict_proba(A2))\n",
    "BPval =  pd.DataFrame(emb_model.predict(B2_padseq))\n",
    "CPval =  pd.DataFrame(rfc_model.predict_proba(C_val_X))\n",
    "prob_val = pd.concat([APval, BPval, CPval], axis=1)\n",
    "prob_val.columns = [i for i in range(30)]\n",
    "\n",
    "# Target\n",
    "APtarget =  pd.DataFrame(xmodel.predict_proba(A3))\n",
    "BPtarget =  pd.DataFrame(emb_model.predict(B3_padseq))\n",
    "CPtarget =  pd.DataFrame(rfc_model.predict_proba(C_tar_X))\n",
    "prob_target = pd.concat([APtarget, BPtarget, CPtarget], axis=1)\n",
    "prob_target.columns = [i for i in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann_model = keras.Sequential([\n",
    "#     layers.Dense(32, activation='relu',input_dim=30),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(rate=0.3),\n",
    "#     layers.Dense(64, activation='selu'),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(0.3),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(0.3),\n",
    "#     layers.Dense(254, activation='gelu'),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(0.3),\n",
    "#     layers.Dense(10, activation='softmax'),\n",
    "#     # layers.Dense(10)\n",
    "# ])\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=0.03)\n",
    "# ann_model.compile(optimizer=optimizer,\n",
    "#               loss = 'categorical_crossentropy',\n",
    "#               metrics=['accuracy']\n",
    "#               )\n",
    "\n",
    "# # early_stopping = callbacks.EarlyStopping(\n",
    "# #     min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "# #     patience=100, # how many epochs to wait before stopping\n",
    "# #     restore_best_weights=True,\n",
    "# # )\n",
    "# history = ann_model.fit(prob_train, Y1, \n",
    "#           validation_data= (prob_val, Y2),\n",
    "#           epochs=1000, batch_size=40, \n",
    "#         #   callbacks=early_stopping,\n",
    "#           verbose=1\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = pd.DataFrame(history.history)\n",
    "# hist.head(1)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(hist['accuracy'], label='accuracy')\n",
    "# plt.plot(hist['val_accuracy'], label='val_accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> SDG, crazy absolute shit </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07808862, 0.07166502, 0.01519242, 0.10844095, 0.13543518,\n",
       "        0.03289309, 0.16695353, 0.01929295, 0.27605326, 0.09598498]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.dirichlet(np.ones(10),size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036821</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>0.035704</td>\n",
       "      <td>0.034449</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.752507</td>\n",
       "      <td>0.025350</td>\n",
       "      <td>0.026950</td>\n",
       "      <td>0.028595</td>\n",
       "      <td>0.021783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122892</td>\n",
       "      <td>0.049396</td>\n",
       "      <td>0.124803</td>\n",
       "      <td>0.090795</td>\n",
       "      <td>0.082879</td>\n",
       "      <td>0.077726</td>\n",
       "      <td>0.088613</td>\n",
       "      <td>0.094203</td>\n",
       "      <td>0.192551</td>\n",
       "      <td>0.076143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.134706</td>\n",
       "      <td>0.051699</td>\n",
       "      <td>0.130620</td>\n",
       "      <td>0.126032</td>\n",
       "      <td>0.086741</td>\n",
       "      <td>0.081348</td>\n",
       "      <td>0.105953</td>\n",
       "      <td>0.098594</td>\n",
       "      <td>0.104615</td>\n",
       "      <td>0.079692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.136510</td>\n",
       "      <td>0.052391</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.127719</td>\n",
       "      <td>0.087902</td>\n",
       "      <td>0.082437</td>\n",
       "      <td>0.093985</td>\n",
       "      <td>0.099914</td>\n",
       "      <td>0.106015</td>\n",
       "      <td>0.080758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.136510</td>\n",
       "      <td>0.052391</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.127719</td>\n",
       "      <td>0.087902</td>\n",
       "      <td>0.082437</td>\n",
       "      <td>0.093985</td>\n",
       "      <td>0.099914</td>\n",
       "      <td>0.106015</td>\n",
       "      <td>0.080758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.136510</td>\n",
       "      <td>0.052391</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.127719</td>\n",
       "      <td>0.087902</td>\n",
       "      <td>0.082437</td>\n",
       "      <td>0.093985</td>\n",
       "      <td>0.099914</td>\n",
       "      <td>0.106015</td>\n",
       "      <td>0.080758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.097274</td>\n",
       "      <td>0.029552</td>\n",
       "      <td>0.069860</td>\n",
       "      <td>0.110637</td>\n",
       "      <td>0.049582</td>\n",
       "      <td>0.422534</td>\n",
       "      <td>0.053013</td>\n",
       "      <td>0.055132</td>\n",
       "      <td>0.059799</td>\n",
       "      <td>0.052616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.101173</td>\n",
       "      <td>0.038829</td>\n",
       "      <td>0.093381</td>\n",
       "      <td>0.122986</td>\n",
       "      <td>0.065148</td>\n",
       "      <td>0.296350</td>\n",
       "      <td>0.069656</td>\n",
       "      <td>0.074050</td>\n",
       "      <td>0.078573</td>\n",
       "      <td>0.059854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.136510</td>\n",
       "      <td>0.052391</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.127719</td>\n",
       "      <td>0.087902</td>\n",
       "      <td>0.082437</td>\n",
       "      <td>0.093985</td>\n",
       "      <td>0.099914</td>\n",
       "      <td>0.106015</td>\n",
       "      <td>0.080758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.136510</td>\n",
       "      <td>0.052391</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.127719</td>\n",
       "      <td>0.087902</td>\n",
       "      <td>0.082437</td>\n",
       "      <td>0.093985</td>\n",
       "      <td>0.099914</td>\n",
       "      <td>0.106015</td>\n",
       "      <td>0.080758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.036821  0.014131  0.035704  0.034449  0.023710  0.752507  0.025350   \n",
       "1    0.122892  0.049396  0.124803  0.090795  0.082879  0.077726  0.088613   \n",
       "2    0.134706  0.051699  0.130620  0.126032  0.086741  0.081348  0.105953   \n",
       "3    0.136510  0.052391  0.132369  0.127719  0.087902  0.082437  0.093985   \n",
       "4    0.136510  0.052391  0.132369  0.127719  0.087902  0.082437  0.093985   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "229  0.136510  0.052391  0.132369  0.127719  0.087902  0.082437  0.093985   \n",
       "230  0.097274  0.029552  0.069860  0.110637  0.049582  0.422534  0.053013   \n",
       "231  0.101173  0.038829  0.093381  0.122986  0.065148  0.296350  0.069656   \n",
       "232  0.136510  0.052391  0.132369  0.127719  0.087902  0.082437  0.093985   \n",
       "233  0.136510  0.052391  0.132369  0.127719  0.087902  0.082437  0.093985   \n",
       "\n",
       "            7         8         9  \n",
       "0    0.026950  0.028595  0.021783  \n",
       "1    0.094203  0.192551  0.076143  \n",
       "2    0.098594  0.104615  0.079692  \n",
       "3    0.099914  0.106015  0.080758  \n",
       "4    0.099914  0.106015  0.080758  \n",
       "..        ...       ...       ...  \n",
       "229  0.099914  0.106015  0.080758  \n",
       "230  0.055132  0.059799  0.052616  \n",
       "231  0.074050  0.078573  0.059854  \n",
       "232  0.099914  0.106015  0.080758  \n",
       "233  0.099914  0.106015  0.080758  \n",
       "\n",
       "[234 rows x 10 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APtarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05811881"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APtrain[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_A(A,B,C):\n",
    "    obs_tot_list = []\n",
    "    for i in range(0,len(A),1):\n",
    "        ptot_list = []\n",
    "        for j  in range(0,10,1):\n",
    "            p1 = A[j][i]\n",
    "            p2  = B[j][i]\n",
    "            p3 = C[j][i]\n",
    "            ptot = sum([p1,p2,p3])\n",
    "            ptot_list.append(ptot)\n",
    "        obs_tot_list.append(ptot_list)\n",
    "    return obs_tot_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_B(P, target, weigths):\n",
    "\n",
    "    label_list = []\n",
    "    for i in range(len(P)):\n",
    "        prob = np.diag(P[i]) @ weigths\n",
    "        label = np.argmax(prob)\n",
    "        label_list.append(label)\n",
    "    score = ACC(label_list, target)\n",
    "    return [score, weigths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allprob_train = part_A(APtrain, BPtrain, CPtrain)\n",
    "# # part_B(allprob_train, A_tra_y )\n",
    "# allprob_val = part_A(APval, BPval, CPval)\n",
    "# # part_B(allprob_train, A_tra_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_C():    \n",
    "    score_df = pd.DataFrame()\n",
    "\n",
    "    scoresTrain = []\n",
    "    scoresVal = []\n",
    "    weigths = []\n",
    "    for i in range(2000):\n",
    "        w = np.random.dirichlet(np.ones(10),size=1).T\n",
    "\n",
    "        first = part_A(APtrain, BPtrain, CPtrain)\n",
    "        second = part_B(first, A_tra_y, w )\n",
    "\n",
    "        third = part_A(APval, BPval, CPval)\n",
    "        fourth = part_B(third, C_val_y, w)\n",
    "        scoresTrain.append(second[0])\n",
    "        scoresVal.append(fourth[0])\n",
    "        weigths.append(second[1])\n",
    "    score_df['scoresTrain'] = scoresTrain\n",
    "    score_df['scoresVal'] = scoresVal\n",
    "    score_df['weigths'] = weigths   \n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "wackatron = part_C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scoresTrain</th>\n",
       "      <th>scoresVal</th>\n",
       "      <th>weigths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.292938</td>\n",
       "      <td>0.212598</td>\n",
       "      <td>[[0.06043969077104463], [0.08811177583527424],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.168265</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>[[0.0003689754779390481], [0.06717935432971474...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.248474</td>\n",
       "      <td>0.161417</td>\n",
       "      <td>[[0.030517391047784146], [0.12405030884601496]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.256321</td>\n",
       "      <td>0.216535</td>\n",
       "      <td>[[0.05001024369543752], [0.017324734218387657]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.260680</td>\n",
       "      <td>0.216535</td>\n",
       "      <td>[[0.20082948732414055], [0.13653597147948449],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.170009</td>\n",
       "      <td>0.200787</td>\n",
       "      <td>[[0.0943959613593153], [0.11954826756939178], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.217088</td>\n",
       "      <td>0.232283</td>\n",
       "      <td>[[0.09674354031027756], [0.0074406696083373644...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.251962</td>\n",
       "      <td>0.208661</td>\n",
       "      <td>[[0.24304932510738453], [0.015609636985957229]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.185039</td>\n",
       "      <td>[[0.046371006192603384], [0.3382541338424892],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.183086</td>\n",
       "      <td>0.129921</td>\n",
       "      <td>[[0.07344391164894372], [0.0971762001817856], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      scoresTrain  scoresVal  \\\n",
       "0        0.292938   0.212598   \n",
       "1        0.168265   0.141732   \n",
       "2        0.248474   0.161417   \n",
       "3        0.256321   0.216535   \n",
       "4        0.260680   0.216535   \n",
       "...           ...        ...   \n",
       "1995     0.170009   0.200787   \n",
       "1996     0.217088   0.232283   \n",
       "1997     0.251962   0.208661   \n",
       "1998     0.193548   0.185039   \n",
       "1999     0.183086   0.129921   \n",
       "\n",
       "                                                weigths  \n",
       "0     [[0.06043969077104463], [0.08811177583527424],...  \n",
       "1     [[0.0003689754779390481], [0.06717935432971474...  \n",
       "2     [[0.030517391047784146], [0.12405030884601496]...  \n",
       "3     [[0.05001024369543752], [0.017324734218387657]...  \n",
       "4     [[0.20082948732414055], [0.13653597147948449],...  \n",
       "...                                                 ...  \n",
       "1995  [[0.0943959613593153], [0.11954826756939178], ...  \n",
       "1996  [[0.09674354031027756], [0.0074406696083373644...  \n",
       "1997  [[0.24304932510738453], [0.015609636985957229]...  \n",
       "1998  [[0.046371006192603384], [0.3382541338424892],...  \n",
       "1999  [[0.07344391164894372], [0.0971762001817856], ...  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wackatron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "wackatron['scoreCommon'] = (wackatron['scoresTrain']+wackatron['scoresVal'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1656], dtype=int64),)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(wackatron['scoreCommon'] == max(wackatron['scoreCommon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scoresTrain                                             0.452485\n",
       "scoresVal                                               0.279528\n",
       "weigths        [[0.1217331983814923], [0.08906155716844878], ...\n",
       "scoreCommon                                             0.366006\n",
       "Name: 1656, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wackatron.iloc[1656]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel = part_A(APtarget,BPtarget,CPtarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18803418803418803,\n",
       " array([[3.68975478e-04],\n",
       "        [6.71793543e-02],\n",
       "        [3.84464539e-01],\n",
       "        [3.06313844e-03],\n",
       "        [2.89104542e-01],\n",
       "        [7.05471019e-02],\n",
       "        [7.64320377e-02],\n",
       "        [1.03942642e-01],\n",
       "        [4.62374962e-03],\n",
       "        [2.73918634e-04]])]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_B(hotel, A_tar_y, wackatron['weigths'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(ptot_list).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 5. Conclusion: </h1>\n",
    "<p> We have used three different datasets trained on three different models. The best individual model is the random forest classifier, which is trained on dummy coded BoW. </p>\n",
    "<br>\n",
    "<p> Furthermore, all the models have been put together in an ensemble model, where the majority class wins. The accuracy of the ensemble model is equal to the accuracy retrieved from the rfc model. This might indicate that there are no documents where the two other models agrees upon another label than the rfc model. In other words; the other models are do not give any type of additional explanatory power other what than the rfc model gives.</p>\n",
    "<br>\n",
    "<p> The upside of the modelling phase is that we have been able to create a model that is better than random guessing by 300% and a model that better than guessing Frodo all the time by approximately 100%. </p>\n",
    "<br>\n",
    "<h1> Biological hazard have left the building at 01:55.  </h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
