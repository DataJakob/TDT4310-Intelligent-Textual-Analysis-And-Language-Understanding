{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jakob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split as TTS,  GridSearchCV  \n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB as NB\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, sentiwordnet, wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import spacy\n",
    "\n",
    "from typing import List\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_train = pd.read_csv('lotr_train.csv')\n",
    "lotr_test = pd.read_csv('lotr_test.csv')\n",
    "imp_char = [\"FRODO\", \"SAM\", \"GANDALF\", \"PIPPIN\", \"MERRY\", \"GOLLUM\", \"GIMLI\", \"THEODEN\", \"FARAMIR\", \"SAURON\", \"ARAGORN\", \"SMEAGOL\"]\n",
    "\n",
    "def common_label_removal(data):\n",
    "    mask = data[\"char\"].isin(imp_char)\n",
    "    data.loc[~ mask, \"char\"] = \"Rest\"\n",
    "    mask2 = data['char'] == 'Rest'\n",
    "    data = data[~mask2]\n",
    "    return data\n",
    "\n",
    "lotr_train = common_label_removal(lotr_train)\n",
    "lotr_test = common_label_removal(lotr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def find_max_length(data):\n",
    "    return max(max(len(nlp(dialogue)) for dialogue in set) for set in data)\n",
    "\n",
    "max_length = find_max_length([lotr_train['dialog'], lotr_test['dialog']])\n",
    "\n",
    "def word2vec_df(data, max_length):\n",
    "    # Extracting norm vector values\n",
    "    word_vectors = []\n",
    "    for dialogue in data:\n",
    "        tokens = nlp(dialogue)\n",
    "        dialogue_vectors = [token.vector_norm for token in tokens]\n",
    "        word_vectors.append(dialogue_vectors)\n",
    "\n",
    "    # Padding \n",
    "    for i in range(len(word_vectors)):\n",
    "        word_vectors[i] += [0] * (max_length - len(word_vectors[i]))\n",
    "\n",
    "    df = pd.DataFrame(word_vectors)\n",
    "    df.columns = [f\"word_{i}\" for i in range(1, max_length + 1)]\n",
    "    return df  \n",
    "\n",
    "train_w2v = word2vec_df(lotr_train['dialog'], max_length)\n",
    "test_w2v = word2vec_df(lotr_test['dialog'], max_length)\n",
    "\n",
    "# train_w2v.to_csv('train_w2v.csv', index=False)\n",
    "# test_w2v.to_csv('test_w2v.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v['char'] = lotr_train['char'].reset_index(drop=True)\n",
    "test_w2v['char'] = lotr_test['char'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_w2v['char'] = label_encoder.fit_transform(train_w2v['char'])\n",
    "test_w2v['char'] = label_encoder.transform(test_w2v['char'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential, layers, Input, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>word_4</th>\n",
       "      <th>word_5</th>\n",
       "      <th>word_6</th>\n",
       "      <th>word_7</th>\n",
       "      <th>word_8</th>\n",
       "      <th>word_9</th>\n",
       "      <th>word_10</th>\n",
       "      <th>...</th>\n",
       "      <th>word_162</th>\n",
       "      <th>word_163</th>\n",
       "      <th>word_164</th>\n",
       "      <th>word_165</th>\n",
       "      <th>word_166</th>\n",
       "      <th>word_167</th>\n",
       "      <th>word_168</th>\n",
       "      <th>word_169</th>\n",
       "      <th>word_170</th>\n",
       "      <th>char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.936516</td>\n",
       "      <td>11.274338</td>\n",
       "      <td>12.535010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.807209</td>\n",
       "      <td>7.082729</td>\n",
       "      <td>10.126812</td>\n",
       "      <td>9.273765</td>\n",
       "      <td>7.891686</td>\n",
       "      <td>8.183380</td>\n",
       "      <td>10.324145</td>\n",
       "      <td>7.371899</td>\n",
       "      <td>9.027041</td>\n",
       "      <td>7.104752</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.937764</td>\n",
       "      <td>7.750165</td>\n",
       "      <td>8.915595</td>\n",
       "      <td>8.662035</td>\n",
       "      <td>6.803154</td>\n",
       "      <td>6.460621</td>\n",
       "      <td>11.343762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.943976</td>\n",
       "      <td>7.265589</td>\n",
       "      <td>10.792604</td>\n",
       "      <td>13.414934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.383319</td>\n",
       "      <td>8.229084</td>\n",
       "      <td>10.143223</td>\n",
       "      <td>7.550912</td>\n",
       "      <td>9.198028</td>\n",
       "      <td>7.539784</td>\n",
       "      <td>7.757065</td>\n",
       "      <td>8.074544</td>\n",
       "      <td>7.358045</td>\n",
       "      <td>7.983031</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>8.870296</td>\n",
       "      <td>9.705757</td>\n",
       "      <td>7.965269</td>\n",
       "      <td>6.652018</td>\n",
       "      <td>8.039692</td>\n",
       "      <td>8.533687</td>\n",
       "      <td>8.299333</td>\n",
       "      <td>11.232112</td>\n",
       "      <td>12.855322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>8.342788</td>\n",
       "      <td>7.922681</td>\n",
       "      <td>8.421476</td>\n",
       "      <td>8.497047</td>\n",
       "      <td>9.581614</td>\n",
       "      <td>10.287869</td>\n",
       "      <td>9.037190</td>\n",
       "      <td>7.900794</td>\n",
       "      <td>11.145840</td>\n",
       "      <td>7.884370</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>9.338175</td>\n",
       "      <td>12.174479</td>\n",
       "      <td>7.489946</td>\n",
       "      <td>10.374016</td>\n",
       "      <td>8.709492</td>\n",
       "      <td>8.717413</td>\n",
       "      <td>8.328237</td>\n",
       "      <td>9.383992</td>\n",
       "      <td>6.920962</td>\n",
       "      <td>7.234212</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>9.798004</td>\n",
       "      <td>8.581070</td>\n",
       "      <td>9.354257</td>\n",
       "      <td>13.826755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>8.133518</td>\n",
       "      <td>10.620996</td>\n",
       "      <td>7.529290</td>\n",
       "      <td>7.960512</td>\n",
       "      <td>9.321564</td>\n",
       "      <td>8.349144</td>\n",
       "      <td>10.677260</td>\n",
       "      <td>9.012439</td>\n",
       "      <td>9.785184</td>\n",
       "      <td>7.911406</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_1     word_2     word_3     word_4    word_5     word_6  \\\n",
       "0      7.936516  11.274338  12.535010   0.000000  0.000000   0.000000   \n",
       "1      7.807209   7.082729  10.126812   9.273765  7.891686   8.183380   \n",
       "2      8.937764   7.750165   8.915595   8.662035  6.803154   6.460621   \n",
       "3     10.943976   7.265589  10.792604  13.414934  0.000000   0.000000   \n",
       "4     11.383319   8.229084  10.143223   7.550912  9.198028   7.539784   \n",
       "...         ...        ...        ...        ...       ...        ...   \n",
       "1147   8.870296   9.705757   7.965269   6.652018  8.039692   8.533687   \n",
       "1148   8.342788   7.922681   8.421476   8.497047  9.581614  10.287869   \n",
       "1149   9.338175  12.174479   7.489946  10.374016  8.709492   8.717413   \n",
       "1150   9.798004   8.581070   9.354257  13.826755  0.000000   0.000000   \n",
       "1151   8.133518  10.620996   7.529290   7.960512  9.321564   8.349144   \n",
       "\n",
       "         word_7     word_8     word_9   word_10  ...  word_162  word_163  \\\n",
       "0      0.000000   0.000000   0.000000  0.000000  ...         0         0   \n",
       "1     10.324145   7.371899   9.027041  7.104752  ...         0         0   \n",
       "2     11.343762   0.000000   0.000000  0.000000  ...         0         0   \n",
       "3      0.000000   0.000000   0.000000  0.000000  ...         0         0   \n",
       "4      7.757065   8.074544   7.358045  7.983031  ...         0         0   \n",
       "...         ...        ...        ...       ...  ...       ...       ...   \n",
       "1147   8.299333  11.232112  12.855322  0.000000  ...         0         0   \n",
       "1148   9.037190   7.900794  11.145840  7.884370  ...         0         0   \n",
       "1149   8.328237   9.383992   6.920962  7.234212  ...         0         0   \n",
       "1150   0.000000   0.000000   0.000000  0.000000  ...         0         0   \n",
       "1151  10.677260   9.012439   9.785184  7.911406  ...         0         0   \n",
       "\n",
       "      word_164  word_165  word_166  word_167  word_168  word_169  word_170  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         0         0         0         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            0         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1147         0         0         0         0         0         0         0   \n",
       "1148         0         0         0         0         0         0         0   \n",
       "1149         0         0         0         0         0         0         0   \n",
       "1150         0         0         0         0         0         0         0   \n",
       "1151         0         0         0         0         0         0         0   \n",
       "\n",
       "      char  \n",
       "0        2  \n",
       "1        8  \n",
       "2        7  \n",
       "3        7  \n",
       "4        0  \n",
       "...    ...  \n",
       "1147     3  \n",
       "1148     7  \n",
       "1149     3  \n",
       "1150     8  \n",
       "1151     0  \n",
       "\n",
       "[1152 rows x 171 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X = train_w2v.loc[:,:'word_170']\n",
    "tr_y = train_w2v.loc[:,'char']\n",
    "te_X = test_w2v.loc[:,:'word_170']\n",
    "te_y = test_w2v.loc[:,'char']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1163 - loss: 5.5368e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 2/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1128 - loss: 5.5325e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 3/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1097 - loss: 5.7176e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 4/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1104 - loss: 5.6870e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 5/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1276 - loss: 5.5326e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 6/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1220 - loss: 5.7429e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 7/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1264 - loss: 5.4863e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 8/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1172 - loss: 5.7088e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 9/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1082 - loss: 5.5981e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 10/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1142 - loss: 5.7118e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 11/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1059 - loss: 5.6121e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 12/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1075 - loss: 5.6701e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 13/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1167 - loss: 5.5057e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 14/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1159 - loss: 5.5771e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 15/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1148 - loss: 5.5680e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 16/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1170 - loss: 5.5482e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 17/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1169 - loss: 5.5524e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 18/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1090 - loss: 5.4290e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 19/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.0961 - loss: 5.8184e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 20/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1163 - loss: 5.6339e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 21/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1062 - loss: 5.7387e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 22/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1163 - loss: 5.7933e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 23/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1096 - loss: 5.5918e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 24/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1060 - loss: 5.6022e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 25/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1296 - loss: 5.6424e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 26/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1206 - loss: 5.4106e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 27/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1068 - loss: 5.6047e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 28/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1057 - loss: 5.7632e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 29/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1188 - loss: 5.6293e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 30/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1183 - loss: 5.5419e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n",
      "Epoch 31/200\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1188 - loss: 5.6483e-07 - val_accuracy: 0.1082 - val_loss: 5.8048e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c249f436d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(64,activation='relu', input_dim=170),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(rate=0.3),\n",
    "    layers.Dense(128, activation='selu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(254, activation='softmax'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=30, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "model.fit(tr_X, tr_y, \n",
    "          validation_data= [te_X,te_y],\n",
    "          epochs=200, batch_size=3, callbacks=early_stopping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1226 - loss: 5.6045e-07\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(tr_X, tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(tr_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_classes\u001b[49m(tr_X)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "model.predict_classes(tr_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
