{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Import libraries and datasets </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jakob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split as TTS,  GridSearchCV  \n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB as NB\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, sentiwordnet, wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import spacy\n",
    "\n",
    "from typing import List\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/lotr_train.csv')\n",
    "test = pd.read_csv('data/lotr_test.csv')\n",
    "\n",
    "imp_char = [\"FRODO\", \"SAM\", \"GANDALF\", \"PIPPIN\", \"MERRY\", \"GOLLUM\", \"GIMLI\", \"THEODEN\", \"FARAMIR\", \"SAURON\", \"ARAGORN\", \"SMEAGOL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character prediction\n",
    "### Divide and conquer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'SAURON', 'SMEAGOL', 'THEODEN']\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'SAURON', 'SMEAGOL', 'THEODEN']\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n"
     ]
    }
   ],
   "source": [
    "# Creating a common label for the characters not of interest. \n",
    "# Aware that this will impact the model, unsure if it would be positive or negative\n",
    "\n",
    "\n",
    "def common_label_removal(data):\n",
    "    mask = data[\"char\"].isin(imp_char)\n",
    "    data.loc[~ mask, \"char\"] = \"Rest\"\n",
    "    mask2 = data['char'] == 'Rest'\n",
    "    data = data[~mask2]\n",
    "    return data\n",
    "\n",
    "train = common_label_removal(train)\n",
    "test = common_label_removal(test)\n",
    "\n",
    "\n",
    "def x_y_split(data):\n",
    "    y_data = data['char']\n",
    "    x_data = data.drop(columns=['char', 'dialog'])\n",
    "    return x_data, y_data\n",
    "\n",
    "X_train_org, y_train_org = x_y_split(train)\n",
    "X_test_org, y_test_org = x_y_split(test)\n",
    "\n",
    "def char_2_num(y_data):\n",
    "    encoder = LabelEncoder()\n",
    "    y_data = y_data.values.reshape(-1, 1)\n",
    "    encoded_data = encoder.fit_transform(y_data)\n",
    "    names = list(encoder.inverse_transform(np.unique(encoded_data)))\n",
    "    print(names)\n",
    "    print(np.unique(encoded_data))\n",
    "    return encoded_data, names\n",
    "\n",
    "y_train_org, names = char_2_num(y_train_org)\n",
    "y_test_org = char_2_num(y_test_org)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.07      0.08       136\n",
      "           1       0.04      0.08      0.05        51\n",
      "           2       0.08      0.05      0.06       167\n",
      "           3       0.14      0.09      0.11       138\n",
      "           4       0.04      0.05      0.05        84\n",
      "           5       0.09      0.09      0.09       102\n",
      "           6       0.10      0.08      0.09       102\n",
      "           7       0.17      0.13      0.15       119\n",
      "           8       0.13      0.08      0.10       143\n",
      "           9       0.01      0.20      0.02         5\n",
      "          10       0.01      0.03      0.02        31\n",
      "          11       0.06      0.08      0.07        74\n",
      "\n",
      "    accuracy                           0.08      1152\n",
      "   macro avg       0.08      0.09      0.07      1152\n",
      "weighted avg       0.10      0.08      0.09      1152\n",
      "\n",
      "Accuracy  0.079\n",
      "f1_score 0.0856\n",
      "precision_score 0.0992\n",
      "recall_score 0.079\n"
     ]
    }
   ],
   "source": [
    "eval_methods = [ f1_score, precision_score, recall_score]\n",
    "\n",
    "def naive_model(x_data, y_data):\n",
    "    pred = np.random.randint(0, 12, size=len(x_data))\n",
    "    print(classification_report(y_data, pred))\n",
    "    print(\"Accuracy \", round(accuracy_score(y_data, pred), 4)) \n",
    "    \n",
    "    for e in eval_methods:\n",
    "        print(str(e.__name__), round(e(y_data, pred, average='weighted'), 4)) \n",
    "    return pred   \n",
    "\n",
    "naive_predicitons = naive_model(X_train_org, y_train_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "ON DATASET \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 15, 'n_estimators': 85, 'random_state': 42}\n",
      "Best Accuracy Score: 0.22484848484848485\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def optimize_model_parameters(X, y, model, param_grid, cv=5):\n",
    " \n",
    "    rfc = model()\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Accuracy Score:\", best_score)\n",
    "\n",
    "\n",
    "    optimized = model(**best_params)\n",
    "    optimized.fit(X, y)\n",
    "    return optimized\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [30,35,45,55,65,75,85,95],\n",
    "    'max_depth': [6,9,12,15,18,21,24,27,30],\n",
    "    'random_state':[42]\n",
    "    # 'min_samples_split': [ 5, 10, 15],\n",
    "    # #'min_samples_leaf': [1, 2, 3, 4],\n",
    "    # 'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Optimize parameters\n",
    "optimized_rf = optimize_model_parameters(X_train_org, y_train_org, RFC, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.17      0.15        53\n",
      "           1       0.00      0.00      0.00        14\n",
      "           2       0.25      0.35      0.29        74\n",
      "           3       0.29      0.36      0.32        76\n",
      "           4       0.12      0.10      0.11        31\n",
      "           5       0.05      0.03      0.04        32\n",
      "           6       0.15      0.14      0.14        35\n",
      "           7       0.11      0.11      0.11        44\n",
      "           8       0.25      0.23      0.24        75\n",
      "           9       0.00      0.00      0.00         2\n",
      "          10       0.17      0.06      0.08        18\n",
      "          11       0.05      0.03      0.04        36\n",
      "\n",
      "    accuracy                           0.19       490\n",
      "   macro avg       0.13      0.13      0.13       490\n",
      "weighted avg       0.18      0.19      0.18       490\n",
      "\n",
      "Accuracy  0.1939\n",
      "f1_score 0.1817\n",
      "precision_score 0.1766\n",
      "recall_score 0.1939\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(x_data, y_data, model):\n",
    "\n",
    "    pred = model.predict(x_data)\n",
    "    print(classification_report(y_data, pred))\n",
    "    print(\"Accuracy \", round(accuracy_score(y_data, pred), 4)) \n",
    "    \n",
    "    for e in eval_methods:\n",
    "        print(str(e.__name__), round(e(y_data, pred, average='weighted'), 4)) \n",
    "    return pred  \n",
    "rfc_predictions = evaluate_model(lotr_test_X, lotr_test_Y, optimized_rf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(y, pred):\n",
    "    cm = confusion_matrix(y, pred)\n",
    "    fig, ax = plt.subplots(figsize=(10,10)) \n",
    "    sns.heatmap(cm/np.sum(cm), annot=True, \n",
    "                fmt='.1%', cmap='Blues', ax=ax, \n",
    "                xticklabels=names, yticklabels=names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.show()\n",
    "\n",
    "# conf_matrix(lotr_test_Y, rfc_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ON DATASET \"A+B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_w2v = pd.read_csv('data/test_w2v.csv')\n",
    "train_w2v = pd.read_csv('data/train_w2v.csv')\n",
    "\n",
    "\n",
    "X_train_org.reset_index(drop=True, inplace=True)\n",
    "X_test_org.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train_all = pd.concat([X_train_org, train_w2v], axis=1)\n",
    "X_test_all = pd.concat([X_test_org, test_w2v], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 15, 'n_estimators': 85, 'random_state': 42}\n",
      "Best Accuracy Score: 0.22484848484848485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.17      0.15        53\n",
      "           1       0.00      0.00      0.00        14\n",
      "           2       0.25      0.35      0.29        74\n",
      "           3       0.29      0.36      0.32        76\n",
      "           4       0.12      0.10      0.11        31\n",
      "           5       0.05      0.03      0.04        32\n",
      "           6       0.15      0.14      0.14        35\n",
      "           7       0.11      0.11      0.11        44\n",
      "           8       0.25      0.23      0.24        75\n",
      "           9       0.00      0.00      0.00         2\n",
      "          10       0.17      0.06      0.08        18\n",
      "          11       0.05      0.03      0.04        36\n",
      "\n",
      "    accuracy                           0.19       490\n",
      "   macro avg       0.13      0.13      0.13       490\n",
      "weighted avg       0.18      0.19      0.18       490\n",
      "\n",
      "Accuracy  0.1939\n",
      "f1_score 0.1817\n",
      "precision_score 0.1766\n",
      "recall_score 0.1939\n"
     ]
    }
   ],
   "source": [
    "optimized_rf2 = optimize_model_parameters(X_train_org, y_train_org, RFC, param_grid)\n",
    "rfc2_predictions = evaluate_model(X_test_org, y_test_org, optimized_rf2) \n",
    "# conf_matrix(lotr_test_Y, rfc2_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Feedforward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras import Sequential, layers, Input, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.eye(12)[y_train_org]\n",
    "y2 = np.eye(12)[y_test_org]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      2\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m8\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m190\u001b[39m),\n\u001b[0;32m      3\u001b[0m     layers\u001b[38;5;241m.\u001b[39mBatchNormalization(),\n\u001b[0;32m      4\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDropout(rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m),\n\u001b[0;32m      5\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m8\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# layers.BatchNormalization(),\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# layers.Dropout(0.3),\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# layers.Dense(254, activation='softmax'),\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m     10\u001b[0m ])\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m               loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m               )\n\u001b[0;32m     16\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m callbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m     17\u001b[0m     min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;66;03m# minimium amount of change to count as an improvement\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m35\u001b[39m, \u001b[38;5;66;03m# how many epochs to wait before stopping\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     20\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(8, activation='relu',input_dim=190),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(rate=0.3),\n",
    "    layers.Dense(8, activation='selu'),\n",
    "    # layers.BatchNormalization(),\n",
    "    # layers.Dropout(0.3),\n",
    "    # layers.Dense(254, activation='softmax'),\n",
    "    layers.Dense(12)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=35, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "model.fit(X_train_all, y1, \n",
    "          validation_data= (X_test_all, y2),\n",
    "        # validation_split=0.3,\n",
    "          epochs=200, batch_size=5, \n",
    "          callbacks=early_stopping\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0976 - loss: 7.4321 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.598530292510986, 0.11224489659070969]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_all, y2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
