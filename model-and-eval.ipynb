{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Modelling and evaluation </h1>\n",
    "<h2> 1. Import and download </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score as ACC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import RNN, Dense, Dropout, BatchNormalization\n",
    "from keras import Sequential, layers, Input, callbacks\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the datasets\n",
    "train_A = pd.read_csv('data/train_A.csv')\n",
    "train_B = pd.read_csv('data/train_B.csv')\n",
    "train_C = pd.read_csv('data/train_C.csv')\n",
    "\n",
    "val_A = pd.read_csv('data/val_A.csv')\n",
    "val_B = pd.read_csv('data/val_B.csv')\n",
    "val_C = pd.read_csv('data/val_C.csv')\n",
    "\n",
    "test_A = pd.read_csv('data/test_A.csv')\n",
    "test_B = pd.read_csv('data/test_B.csv')\n",
    "test_C = pd.read_csv('data/test_C.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Data preprocessing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train_A, val_A, test_A, \n",
    "            train_B, val_B, test_B, \n",
    "            train_C, val_C, test_C]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "imp_char = [\"FRODO\", \"SAM\", \"GANDALF\", \"PIPPIN\", \"MERRY\", \"GOLLUM\", \"GIMLI\", \"THEODEN\", \"FARAMIR\", \"ARAGORN\"]\n",
    "\n",
    "# Creating a common label for the characters not of interest\n",
    "def common_label_removal(data):\n",
    "    mask = data[\"char\"].isin(imp_char)\n",
    "    data.loc[~ mask, \"char\"] = \"Rest\"\n",
    "    mask2 = data['char'] == 'Rest'\n",
    "    data = data[~mask2]\n",
    "    return data\n",
    "\n",
    "def x_y_split(data):\n",
    "    y_data = data['char']\n",
    "    x_data = data.drop(columns=['char', 'dialog'])\n",
    "    return x_data, y_data\n",
    "\n",
    "def char_2_num(y_data):\n",
    "    encoder = LabelEncoder()\n",
    "    y_data = y_data.values.reshape(-1, 1)\n",
    "    encoded_data = encoder.fit_transform(y_data)\n",
    "    names = list(encoder.inverse_transform(np.unique(encoded_data)))\n",
    "    print(names)\n",
    "    print(np.unique(encoded_data))\n",
    "    return encoded_data, names\n",
    "\n",
    "def preprocessing(data):\n",
    "    data = common_label_removal(data)\n",
    "    x_data, y_data = x_y_split(data)\n",
    "    y_data = char_2_num(y_data)\n",
    "    return x_data, y_data\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    datasets[i] = preprocessing(datasets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_tra_X =datasets[0][0]\n",
    "A_tra_y =datasets[0][1][0]\n",
    "A_val_X =datasets[1][0]\n",
    "A_tra_y=datasets[1][1][0]\n",
    "A_tar_X=datasets[2][0]\n",
    "A_tar_y=datasets[2][1][0]\n",
    "\n",
    "B_tra_X =datasets[3][0]\n",
    "B_tra_y =datasets[3][1][0]\n",
    "B_val_X =datasets[4][0]\n",
    "B_val_y=datasets[4][1][0]\n",
    "B_tar_X=datasets[5][0]\n",
    "B_tar_y=datasets[5][1][0]\n",
    "\n",
    "C_tra_X =datasets[6][0]\n",
    "C_tra_y =datasets[6][1][0]\n",
    "C_val_X =datasets[7][0]\n",
    "C_val_y=datasets[7][1][0]\n",
    "C_tar_X=datasets[8][0]\n",
    "C_tar_y=datasets[8][1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Benchmarks </h2>\n",
    "<h3> 2.1 Naive Benchmark, Monte Carlo Method </h3>\n",
    "<p> Using 1000 simulations with random guesses on target labels. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_benchmark_MonC(y):\n",
    "    accuracy_list = []\n",
    "    for i in range(0,1000,1):\n",
    "        naive_rand_pred = np.random.randint(0,12,size=(len(y)))\n",
    "        accuracy_sel = ACC(naive_rand_pred, y)\n",
    "        accuracy_list.append(accuracy_sel)\n",
    "    return np.mean(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08300854700854701"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_benchmark_MonC(A_tar_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2 Naive Benchmark, Majority Class Method </h3>\n",
    "<p> Using Frodo, which equals label 2, as guess </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_benchmark_MajC(y):\n",
    "    pred_MCNB =np.repeat(2,len(y))\n",
    "    return ACC(pred_MCNB, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1752136752136752"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_benchmark_MajC(A_tar_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Modelling  </h2>\n",
    "<h3> 3.1 ANN on dataset A</h3>\n",
    "<p> Dataset A contains various numerical retrieved from the characters. </p>\n",
    "<p> The feedforward neural network has a relative simple architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "A1 = scaler.fit_transform(A_tra_X)\n",
    "A2 = scaler.transform(A_val_X)\n",
    "A3 = scaler.transform(A_tar_X)\n",
    "\n",
    "Y1 = np.eye(10)[A_tra_y]\n",
    "Y2 = np.eye(10)[A_tra_y]\n",
    "Y3 = np.eye(10)[A_tar_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25bb1fcdd90>"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_model = keras.Sequential([\n",
    "    layers.Dense(8, activation='relu',input_dim=20),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(rate=0.3),\n",
    "    # layers.Dense(16, activation='selu'),\n",
    "    # layers.BatchNormalization(),\n",
    "    # layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax'),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "ann_model.compile(optimizer=optimizer,\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=35, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "ann_model.fit(A1, Y1, \n",
    "          validation_data= (A2, Y2),\n",
    "          epochs=200, batch_size=10, \n",
    "          callbacks=early_stopping,\n",
    "          verbose=0\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1093 - loss: 7.5492 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.387547016143799, 0.11682650446891785]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy\n",
    "ann_model.evaluate(A1, Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1093 - loss: 7.5492  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.387547016143799, 0.11682650446891785]"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation accuracy\n",
    "ann_model.evaluate(A2, Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0312 - loss: 9.0664"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0798 - loss: 7.6752 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.654247283935547, 0.11538461595773697]"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test accuracy\n",
    "ann_model.evaluate(A3, Y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.2 RNN on dataset B </h3>\n",
    "<p> Dataset B contains embeddings(?). This, I need to read myself up on.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Embedding,Dense\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input\n",
    "from keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = pd.read_csv('data/train_df.csv')\n",
    "B2= pd.read_csv('data/val_df.csv')\n",
    "B3 = pd.read_csv('data/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = common_label_removal(B1).reset_index(drop=True)\n",
    "B2 = common_label_removal(B2).reset_index(drop=True)\n",
    "B3 = common_label_removal(B3).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quote_list(X):\n",
    "    quote_list = []\n",
    "    for quote in range(len(X)):\n",
    "        splitted_quote =  X['dialog'][quote].split()\n",
    "        sequence_list = []\n",
    "        for split in range(len(splitted_quote)):\n",
    "            splitted_word = splitted_quote[split]\n",
    "\n",
    "            word_list = str()\n",
    "            i=0\n",
    "            while i < (len(splitted_word)):\n",
    "                # print(splitted_word[i])\n",
    "                if splitted_word[i].isalpha() == True:\n",
    "                    word_list += splitted_word[i]\n",
    "                i+=1\n",
    "            sequence_list.append(word_list)\n",
    "        quote_list.append(sequence_list)\n",
    "    return quote_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxlen(X):\n",
    "    uni = []\n",
    "    for i in range(len(quote_list)):\n",
    "        for j in range(len(quote_list[i])):\n",
    "            if quote_list[i][j] not in uni:\n",
    "                uni.append(quote_list[i][j])\n",
    "    return len(uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = quote_list(B1)\n",
    "B2 = quote_list(B2)\n",
    "B3 = quote_list(B3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(B1)\n",
    "B1_seq = tokenizer.texts_to_sequences(B1)\n",
    "B2_seq = tokenizer.texts_to_sequences(B2)\n",
    "B3_seq = tokenizer.texts_to_sequences(B3)\n",
    "maxlen = max([len(seq) for seq in B1_seq])\n",
    "\n",
    "\n",
    "B1_padseq = pad_sequences(B1_seq, maxlen=maxlen,padding='post')\n",
    "B2_padseq = pad_sequences(B2_seq, maxlen=maxlen,padding='post')\n",
    "B3_padseq = pad_sequences(B3_seq, maxlen=maxlen,padding='post')\n",
    "\n",
    "B1y = np.eye(10)[B_tra_y]\n",
    "B2y = np.eye(10)[B_val_y]\n",
    "B3y = np.eye(10)[B_tar_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1068 - loss: 2.4131 - val_accuracy: 0.0748 - val_loss: 2.4715\n",
      "Epoch 2/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1874 - loss: 2.2110 - val_accuracy: 0.2480 - val_loss: 2.3005\n",
      "Epoch 3/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2620 - loss: 2.0692 - val_accuracy: 0.2283 - val_loss: 2.5706\n",
      "Epoch 4/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2959 - loss: 1.9750 - val_accuracy: 0.2008 - val_loss: 3.3232\n",
      "Epoch 5/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3537 - loss: 1.8835 - val_accuracy: 0.1654 - val_loss: 3.1006\n",
      "Epoch 6/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3784 - loss: 1.8179 - val_accuracy: 0.2165 - val_loss: 2.3450\n",
      "Epoch 7/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3730 - loss: 1.7804 - val_accuracy: 0.2047 - val_loss: 2.4815\n",
      "Epoch 8/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4175 - loss: 1.6994 - val_accuracy: 0.2165 - val_loss: 3.0420\n",
      "Epoch 9/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4352 - loss: 1.6229 - val_accuracy: 0.2520 - val_loss: 2.4166\n",
      "Epoch 10/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4239 - loss: 1.6420 - val_accuracy: 0.2244 - val_loss: 2.3448\n",
      "Epoch 11/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4445 - loss: 1.6206 - val_accuracy: 0.1929 - val_loss: 2.7025\n",
      "Epoch 12/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4337 - loss: 1.5965 - val_accuracy: 0.1969 - val_loss: 2.6252\n",
      "Epoch 13/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4470 - loss: 1.5672 - val_accuracy: 0.2441 - val_loss: 2.4633\n",
      "Epoch 14/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4433 - loss: 1.6005 - val_accuracy: 0.1850 - val_loss: 2.5126\n",
      "Epoch 15/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4816 - loss: 1.4657 - val_accuracy: 0.1614 - val_loss: 3.1583\n",
      "Epoch 16/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4981 - loss: 1.4755 - val_accuracy: 0.1732 - val_loss: 3.1221\n",
      "Epoch 17/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4947 - loss: 1.4546 - val_accuracy: 0.2323 - val_loss: 2.6504\n",
      "Epoch 18/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5054 - loss: 1.3989 - val_accuracy: 0.2087 - val_loss: 2.6812\n",
      "Epoch 19/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5182 - loss: 1.4041 - val_accuracy: 0.2520 - val_loss: 2.6730\n",
      "Epoch 20/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5416 - loss: 1.3335 - val_accuracy: 0.2559 - val_loss: 2.7791\n",
      "Epoch 21/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5459 - loss: 1.3340 - val_accuracy: 0.1969 - val_loss: 3.5239\n",
      "Epoch 22/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5080 - loss: 1.3940 - val_accuracy: 0.2323 - val_loss: 2.7501\n",
      "Epoch 23/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5349 - loss: 1.3294 - val_accuracy: 0.2480 - val_loss: 2.5324\n",
      "Epoch 24/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5244 - loss: 1.3866 - val_accuracy: 0.2087 - val_loss: 2.5553\n",
      "Epoch 25/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5539 - loss: 1.2876 - val_accuracy: 0.2283 - val_loss: 2.6063\n",
      "Epoch 26/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5506 - loss: 1.2478 - val_accuracy: 0.2402 - val_loss: 2.8840\n",
      "Epoch 27/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5325 - loss: 1.3786 - val_accuracy: 0.2087 - val_loss: 2.7407\n",
      "Epoch 28/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5910 - loss: 1.2699 - val_accuracy: 0.2480 - val_loss: 2.8800\n",
      "Epoch 29/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5542 - loss: 1.2837 - val_accuracy: 0.2402 - val_loss: 2.5749\n",
      "Epoch 30/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5741 - loss: 1.2487 - val_accuracy: 0.2323 - val_loss: 2.7342\n",
      "Epoch 31/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5679 - loss: 1.2236 - val_accuracy: 0.2244 - val_loss: 2.8623\n",
      "Epoch 32/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5642 - loss: 1.3327 - val_accuracy: 0.2362 - val_loss: 2.8923\n",
      "Epoch 33/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5579 - loss: 1.2277 - val_accuracy: 0.2441 - val_loss: 2.6673\n",
      "Epoch 34/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5678 - loss: 1.2654 - val_accuracy: 0.1969 - val_loss: 2.8417\n",
      "Epoch 35/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5335 - loss: 1.3411 - val_accuracy: 0.1575 - val_loss: 4.7428\n",
      "Epoch 36/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5451 - loss: 1.3343 - val_accuracy: 0.2480 - val_loss: 2.7086\n",
      "Epoch 37/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5340 - loss: 1.3959 - val_accuracy: 0.2126 - val_loss: 2.9286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25b96bc4110>"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model = Sequential([\n",
    "    layers.Embedding(input_dim=86, output_dim=15, input_length=maxlen),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(16, activation='selu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.03)\n",
    "emb_model.compile(optimizer=optimizer, \n",
    "            loss='categorical_crossentropy', \n",
    "            metrics=['accuracy'])\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=35, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "emb_model.fit(B1_padseq,B1y, epochs=100, batch_size=10, \n",
    "        validation_data=(B2_padseq, B2y),\n",
    "        callbacks=early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2051 - loss: 2.2539 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2088305950164795, 0.2197035700082779]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy\n",
    "emb_model.evaluate(B1_padseq, B1y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2406 - loss: 2.3796 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3005383014678955, 0.24803149700164795]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation accuracy\n",
    "emb_model.evaluate(B2_padseq, B2y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2110 - loss: 2.2559 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.315434217453003, 0.1794871836900711]"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test accuracy\n",
    "emb_model.evaluate(B3_padseq, B3y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> sources </p>\n",
    "<ul>\n",
    "<li>https://keras.io/api/models/model/</li>\n",
    "<li>https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456</li>\n",
    "<li>https://www.kaggle.com/code/rajmehra03/a-detailed-explanation-of-keras-embedding-layer</li>\n",
    "<li>https://medium.com/@iqra.bismi/understanding-keras-embedding-for-natural-language-processing-9f65a281b1a7</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.3 RFC on dataset C </h3>\n",
    "<p>  Dataset C contains a counter on how many times a specific word have been mentioned in a quote. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [30,35,45,55,65,75,85,95],\n",
    "    'max_depth': [6,9,12,15,18,21,24,27,30],\n",
    "}\n",
    "\n",
    "acc_list = []\n",
    "for n in range(len(param_grid['n_estimators'])):\n",
    "    nE = param_grid['n_estimators'][n]\n",
    "    for d in range(len(param_grid['max_depth'])):\n",
    "        mD = param_grid['max_depth'][d]\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=nE, max_depth=mD, random_state=42) \n",
    "        model.fit(C_tra_X,C_tra_y)\n",
    "        X1 = model.predict(C_tra_X)\n",
    "        x2 = model.predict(C_val_X)\n",
    "        acc_list.append(ACC(x2, C_val_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([59], dtype=int64),)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series(acc_list)\n",
    "np.where(a==max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ne 85\n",
    "#md 24\n",
    "rfc_model = RandomForestClassifier(n_estimators=55, max_depth=15)\n",
    "rfc_model.fit(C_tra_X,C_tra_y)\n",
    "predCtrain= rfc_model.predict(C_tra_X)\n",
    "predCval= rfc_model.predict(C_val_X)\n",
    "predCtest= rfc_model.predict(C_tar_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5475152571926766"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy \n",
    "ACC(predCtrain, C_tra_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32677165354330706"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy \n",
    "ACC(predCval, C_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32905982905982906"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy \n",
    "ACC(predCtest, C_tar_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Ensemble model </h2>\n",
    "<p> The RFC contains absolutely best results therefore, they will have prioritized votes if there are ties. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann_model\n",
    "# emb_model\n",
    "# rfc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step  \n"
     ]
    }
   ],
   "source": [
    "pp = ann_model.predict(A3)\n",
    "P1 = pp.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "pp = ann_model.predict(A3)\n",
    "P1 = pp.argmax(axis=1)\n",
    "\n",
    "pp = emb_model.predict(B3_padseq)\n",
    "P2 = pp.argmax(axis=1)\n",
    "\n",
    "P3 = rfc_model.predict(C_tar_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = []\n",
    "for i in range(len(P1)):\n",
    "    preds =  [P1[i],P2[i],P3[i]]\n",
    "    if preds[0]==preds[1]:\n",
    "        ans = preds[0]\n",
    "    elif preds[0]==preds[2]:\n",
    "        ans= preds[0]\n",
    "    elif preds[1]==preds[2]:\n",
    "        ans=preds[1]\n",
    "    else:\n",
    "        ans = preds[2]\n",
    "    final_preds.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32905982905982906"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACC(final_preds, A_tar_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 5. Conclusion: </h1>\n",
    "<p> We have used three different datasets trained on three different models. The best individual model is the random forest classifier, which is trained on dummy coded BoW. </p>\n",
    "<br>\n",
    "<p> Furthermore, all the models have been put together in an ensemble model, where the majority class wins. The accuracy of the ensemble model is equal to the accuracy retrieved from the rfc model. This might indicate that there are no documents where the two other models agrees upon another label than the rfc model. In other words; the other models are do not give any type of additional explanatory power other what than the rfc model gives.</p>\n",
    "<br>\n",
    "<p> The upside of the modelling phase is that we have been able to create a model that is better than random guessing by 300% and a model that better than guessing Frodo all the time by approximately 100%. </p>\n",
    "<br>\n",
    "<h1> Biological hazard have left the building at 01:55.  </h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
